{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a200acf2",
   "metadata": {},
   "source": [
    "# Deduplication Pipeline\n",
    "This notebook demonstrates an image deduplication workflow using multiple stages:\n",
    "1. MD5 byte comparison\n",
    "2. Perceptual hashing (pHash and dHash)\n",
    "3. CNN based similarity\n",
    "\n",
    "The results of each stage are merged and duplicate images are moved to review folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7402aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, numpy as np, google.protobuf as pb\n",
    "print('TF :', tf.__version__)\n",
    "print('NP :', np.__version__)\n",
    "print('PB :', pb.__version__)\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b18688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import hashlib, json, shutil\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from imagededup.methods import PHash, DHash, CNN\n",
    "from imagededup.utils import plot_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0790746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure directories\n",
    "IMAGE_DIR = Path('E:/DeepLearning/Data/Mika-Pikazo_Full')\n",
    "WORK_DIR = IMAGE_DIR / '_dedup_meta'\n",
    "REVIEW_DIR = IMAGE_DIR / '_duplicates_to_review'\n",
    "\n",
    "for d in (WORK_DIR, REVIEW_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('IMAGE_DIR :', IMAGE_DIR.resolve())\n",
    "print('WORK_DIR  :', WORK_DIR.resolve())\n",
    "print('REVIEW_DIR:', REVIEW_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ad6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5sum(fp, blocksize=1 << 16):\n",
    "    h = hashlib.md5()\n",
    "    with open(fp, 'rb') as f:\n",
    "        for blk in iter(lambda: f.read(blocksize), b''):\n",
    "            h.update(blk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def merge_duplicate_maps(*maps):\n",
    "    merged = defaultdict(set)\n",
    "    for m in maps:\n",
    "        for k, vals in m.items():\n",
    "            for v in vals:\n",
    "                merged[k].add(v)\n",
    "                merged[v].add(k)\n",
    "    return {k: sorted(vs - {k}) for k, vs in merged.items() if vs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435bc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 0: exact file matches using MD5\n",
    "md5_dup_map = defaultdict(list)\n",
    "hash_buckets = defaultdict(list)\n",
    "\n",
    "for img in tqdm(IMAGE_DIR.glob('*')):\n",
    "    if img.is_file():\n",
    "        digest = md5sum(img)\n",
    "        hash_buckets[digest].append(img.name)\n",
    "\n",
    "for file_list in hash_buckets.values():\n",
    "    if len(file_list) > 1:\n",
    "        for f in file_list:\n",
    "            md5_dup_map[f] = [x for x in file_list if x != f]\n",
    "\n",
    "byte_dups = hash_buckets\n",
    "json.dump(md5_dup_map, open(WORK_DIR/'stage0_md5_filename_keys.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97014f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: perceptual hashing\n",
    "phasher, dhasher = PHash(), DHash()\n",
    "\n",
    "ph_enc = phasher.encode_images(image_dir=IMAGE_DIR)\n",
    "dh_enc = dhasher.encode_images(image_dir=IMAGE_DIR)\n",
    "\n",
    "ph_dups = phasher.find_duplicates(encoding_map=ph_enc, max_distance_threshold=2)\n",
    "dh_dups = dhasher.find_duplicates(encoding_map=dh_enc, max_distance_threshold=2)\n",
    "\n",
    "hash_dups = merge_duplicate_maps(ph_dups, dh_dups)\n",
    "json.dump(hash_dups, open(WORK_DIR/'stage1_hash_dups.json', 'w'), indent=2)\n",
    "print(f'Stage-1 produced {len(hash_dups)} duplicate clusters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: CNN similarity search\n",
    "cnn = CNN()\n",
    "all_cnn_enc = cnn.encode_images(image_dir=IMAGE_DIR)\n",
    "cnn_dups = cnn.find_duplicates(\n",
    "    encoding_map=all_cnn_enc,\n",
    "    min_similarity_threshold=0.95,\n",
    "    outfile=str(WORK_DIR/'stage2_cnn_dups.json')\n",
    ")\n",
    "print(f'CNN pass produced {len(cnn_dups)} clusters (whole dir).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results from all stages\n",
    "def merge_maps_union(*maps):\n",
    "    merged = {}\n",
    "    for m in maps:\n",
    "        for k, vals in m.items():\n",
    "            merged.setdefault(k, set()).update(vals)\n",
    "    return {k: sorted(v - {k}) for k, v in merged.items() if v}\n",
    "\n",
    "merged_dups = merge_maps_union(md5_dup_map, hash_dups, cnn_dups)\n",
    "print(f'Union map clusters: {len(merged_dups)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19397923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert duplicate map to cluster list\n",
    "def dict_to_clusters(dup_map):\n",
    "    clusters, seen = [], set()\n",
    "    for k, v in dup_map.items():\n",
    "        if k in seen:\n",
    "            continue\n",
    "        c = {k, *v}\n",
    "        clusters.append(sorted(c))\n",
    "        seen.update(c)\n",
    "    return clusters\n",
    "\n",
    "clusters = dict_to_clusters(merged_dups)\n",
    "print(f'Total clusters to review: {len(clusters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few clusters\n",
    "dup_map = merged_dups\n",
    "visited = set()\n",
    "shown = 0\n",
    "MAX_SHOWN = 5\n",
    "\n",
    "print('\n",
    "=== DUPLICATE GROUPS ===')\n",
    "for root, group in dup_map.items():\n",
    "    if root in visited:\n",
    "        continue\n",
    "    plot_duplicates(str(IMAGE_DIR), dup_map, filename=root)\n",
    "    print(f'Cluster size = {len(group) + 1}')\n",
    "    visited.update(group)\n",
    "    visited.add(root)\n",
    "    shown += 1\n",
    "    if shown >= MAX_SHOWN:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b92aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move duplicates to review folders\n",
    "def image_resolution(path: Path) -> int:\n",
    "    with Image.open(path) as im:\n",
    "        w, h = im.size\n",
    "    return w * h\n",
    "\n",
    "moved_files = []\n",
    "for idx, cluster in enumerate(clusters, 1):\n",
    "    live = [fn for fn in cluster if (IMAGE_DIR / fn).exists()]\n",
    "    if not live:\n",
    "        print(f'Cluster {idx:04d} already processed, skip.')\n",
    "        continue\n",
    "\n",
    "    best_img = max(\n",
    "        live,\n",
    "        key=lambda fn: (\n",
    "            image_resolution(IMAGE_DIR / fn),\n",
    "            (IMAGE_DIR / fn).stat().st_size,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    group_dir = REVIEW_DIR / f'cluster_{idx:04d}'\n",
    "    group_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for fn in live:\n",
    "        src = IMAGE_DIR / fn\n",
    "        dst = group_dir / fn\n",
    "        if fn == best_img:\n",
    "            shutil.copy2(src, dst)\n",
    "        else:\n",
    "            shutil.move(src, dst)\n",
    "            moved_files.append(fn)\n",
    "\n",
    "    print(f'Cluster {idx:04d}: kept {best_img}, moved {len(live)-1} dupes.')\n",
    "\n",
    "print('All clusters processed; review folders ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbf352",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_checked = len(list(IMAGE_DIR.glob('*'))) + len(moved_files)\n",
    "print(f\"\"\"\n",
    "======== SUMMARY ========\n",
    "Images processed : {total_checked}\n",
    "MD5 duplicates   : {len(byte_dups)}\n",
    "Hash duplicates  : {len(hash_dups)}\n",
    "CNN duplicates   : {len(cnn_dups)}\n",
    "Files moved      : {len(moved_files)}\n",
    "Meta saved to    : {WORK_DIR}\n",
    "Dup moved to     : {REVIEW_DIR}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
